# ğŸ“ Lab 2.1 : Loki - AgrÃ©gation de Logs

**DurÃ©e estimÃ©e** : 2 heures  
**Niveau** : IntermÃ©diaire  
**PrÃ©requis** : Lab 1.2 (Installation), Lab 1.4 (Prometheus)

---

## ğŸ¯ Objectifs d'Apprentissage

Ã€ la fin de ce lab, vous serez capable de :

- âœ… Comprendre l'architecture Loki + Promtail
- âœ… Configurer Promtail pour collecter des logs
- âœ… MaÃ®triser le langage LogQL (requÃªtes de logs)
- âœ… CrÃ©er des dashboards de logs dans Grafana
- âœ… CorrÃ©ler logs et mÃ©triques
- âœ… Configurer des filtres et des parsers de logs

---

## ğŸ“‹ PrÃ©requis

### Services Docker Requis

```bash
# VÃ©rifier que les services sont dÃ©marrÃ©s
docker ps | grep -E "loki|promtail|grafana"

# Devrait afficher :
# - loki (port 3100)
# - promtail (collecteur de logs)
# - grafana (port 3000)
```

### AccÃ¨s aux Interfaces

| Service | URL | Credentials |
|---------|-----|-------------|
| **Grafana** | http://localhost:3000 | admin / GrafanaSecure123!Change@Me |
| **Loki** | http://localhost:3100 | - |

---

## ğŸ“š Partie 1 : Architecture Loki (30 min)

### Qu'est-ce que Loki ?

**Loki** est un systÃ¨me d'agrÃ©gation de logs inspirÃ© de Prometheus, conÃ§u pour Ãªtre :
- **Ã‰conomique** : N'indexe que les mÃ©tadonnÃ©es (labels), pas le contenu
- **Scalable** : Stockage objet (S3, MinIO, etc.)
- **IntÃ©grÃ©** : Natif dans Grafana

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application â”‚
â”‚   + Logs    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Push      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Promtail   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚   Loki   â”‚
â”‚ (Collector) â”‚                â”‚ (Server) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â”‚ Query
                                    â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Grafana  â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composants

| Composant | RÃ´le | Port |
|-----------|------|------|
| **Loki** | Serveur d'agrÃ©gation | 3100 |
| **Promtail** | Agent de collecte | - |
| **Grafana** | Visualisation | 3000 |

### DiffÃ©rence avec ELK Stack

| CritÃ¨re | Loki | Elasticsearch |
|---------|------|---------------|
| **Indexation** | Labels uniquement | Full-text |
| **Stockage** | Chunks compressÃ©s | Documents JSON |
| **CoÃ»t** | Faible | Ã‰levÃ© |
| **RequÃªtes** | LogQL (comme PromQL) | Query DSL |
| **IntÃ©gration** | Natif Grafana | Kibana |

---

## ğŸ”§ Partie 2 : Configuration Promtail (45 min)

### VÃ©rifier la Configuration Actuelle

```bash
# Voir les logs de Promtail
docker logs promtail --tail 50

# VÃ©rifier la configuration
docker exec promtail cat /etc/promtail/promtail-config.yaml
```

### CrÃ©er une Configuration Promtail

CrÃ©ez le fichier `promtail/promtail-config.yaml` :

```yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  # Logs systÃ¨me
  - job_name: system
    static_configs:
      - targets:
          - localhost
        labels:
          job: varlogs
          __path__: /var/log/*log

  # Logs Docker
  - job_name: docker
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
    relabel_configs:
      - source_labels: ['__meta_docker_container_name']
        regex: '/(.*)'
        target_label: 'container'
      - source_labels: ['__meta_docker_container_log_stream']
        target_label: 'stream'

  # Logs applicatifs (exemple)
  - job_name: app_logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: app
          env: dev
          __path__: /var/log/app/*.log
    pipeline_stages:
      # Parser JSON
      - json:
          expressions:
            level: level
            message: message
            timestamp: timestamp
      # Extraire le niveau de log
      - labels:
          level:
      # Filtrer les logs DEBUG en dev
      - match:
          selector: '{level="DEBUG"}'
          action: drop
```

### Configuration AvancÃ©e : Pipeline Stages

#### 1. Parser JSON

```yaml
pipeline_stages:
  - json:
      expressions:
        level: level
        message: msg
        user_id: user.id
        request_id: request_id
```

#### 2. Parser Regex

```yaml
pipeline_stages:
  - regex:
      expression: '^(?P<timestamp>\S+) (?P<level>\w+) (?P<message>.*)$'
  - labels:
      level:
  - timestamp:
      source: timestamp
      format: RFC3339
```

#### 3. Filtres

```yaml
pipeline_stages:
  # Garder uniquement ERROR et WARN
  - match:
      selector: '{level!~"ERROR|WARN"}'
      action: drop
  
  # Ajouter un label pour les erreurs critiques
  - match:
      selector: '{level="ERROR"}'
      stages:
        - labels:
            severity: critical
```

### RedÃ©marrer Promtail

```bash
# Windows PowerShell
docker-compose restart promtail

# VÃ©rifier les logs
docker logs promtail --tail 20
```

---

## ğŸ” Partie 3 : LogQL - Langage de RequÃªte (45 min)

### Syntaxe de Base

LogQL est similaire Ã  PromQL avec deux types d'expressions :

1. **Log Queries** : SÃ©lection et filtrage
2. **Metric Queries** : AgrÃ©gation et calculs

### 1. SÃ©lection de Logs

#### SÃ©lection par Labels

```logql
# Tous les logs d'un job
{job="varlogs"}

# Logs d'un container spÃ©cifique
{container="prometheus"}

# Combinaison de labels
{job="app", env="production", level="ERROR"}
```

#### Filtrage par Contenu

```logql
# Contient "error" (case-insensitive)
{job="app"} |= "error"

# Ne contient pas "debug"
{job="app"} != "debug"

# Regex : commence par "ERROR"
{job="app"} |~ "^ERROR.*"

# Regex nÃ©gative
{job="app"} !~ "DEBUG|TRACE"
```

#### Combinaison de Filtres

```logql
# Logs d'erreur contenant "database" ou "connection"
{job="app"} |= "error" |~ "database|connection"

# Logs sans "health check" ni "ping"
{job="app"} != "health check" != "ping"
```

### 2. Parsers

#### Parser JSON

```logql
# Extraire des champs JSON
{job="app"} | json

# Extraire des champs spÃ©cifiques
{job="app"} | json level="level", message="msg", user="user.id"

# Filtrer aprÃ¨s parsing
{job="app"} | json | level="ERROR"
```

#### Parser Logfmt

```logql
# Format: key=value key2=value2
{job="app"} | logfmt

# Filtrer sur un champ parsÃ©
{job="app"} | logfmt | status_code >= 400
```

#### Parser Regex

```logql
# Extraire avec regex
{job="nginx"} | regexp '(?P<ip>\S+) .* "(?P<method>\w+) (?P<path>\S+).*" (?P<status>\d+)'

# Utiliser les champs extraits
{job="nginx"} | regexp '(?P<status>\d+)' | status >= 500
```

### 3. Formatters

```logql
# Formater la sortie
{job="app"} | line_format "{{.level}} - {{.message}}"

# Formater avec conditions
{job="app"} | label_format level="{{if eq .level \"ERROR\"}}ğŸ”´{{else}}âœ…{{end}}"
```

### 4. MÃ©triques de Logs

#### Comptage

```logql
# Nombre de logs par seconde
rate({job="app"}[5m])

# Nombre total de logs
count_over_time({job="app"}[1h])

# Logs d'erreur par minute
sum(rate({job="app", level="ERROR"}[1m]))
```

#### AgrÃ©gation

```logql
# Logs par container
sum by (container) (rate({job="docker"}[5m]))

# Top 5 des containers les plus verbeux
topk(5, sum by (container) (rate({job="docker"}[5m])))

# Ratio d'erreurs
sum(rate({level="ERROR"}[5m])) / sum(rate({job="app"}[5m]))
```

#### MÃ©triques Extraites

```logql
# DurÃ©e moyenne des requÃªtes (depuis logs JSON)
avg_over_time({job="app"} | json | unwrap duration [5m])

# Percentile 95 des temps de rÃ©ponse
quantile_over_time(0.95, {job="nginx"} | regexp '(?P<duration>\d+)ms' | unwrap duration [5m])

# Bytes transfÃ©rÃ©s
sum(rate({job="nginx"} | json | unwrap bytes [5m]))
```

---

## ğŸ¯ Exercice Pratique 1 : RequÃªtes LogQL (30 min)

### Objectif

Explorer les logs de votre stack avec LogQL.

### Ã‰tapes

#### 1. AccÃ©der Ã  l'Explorateur Grafana

```
Grafana â†’ Explore â†’ Datasource: Loki
```

#### 2. RequÃªtes de Base

**Exercice 1.1** : Afficher tous les logs Prometheus

```logql
{container="prometheus"}
```

**Exercice 1.2** : Logs d'erreur uniquement

```logql
{container="prometheus"} |~ "error|ERROR|Error"
```

**Exercice 1.3** : Logs sans "health check"

```logql
{container="grafana"} != "health"
```

#### 3. RequÃªtes AvancÃ©es

**Exercice 1.4** : Taux de logs par container

```logql
sum by (container) (rate({job="docker"}[5m]))
```

**Exercice 1.5** : Top 3 des containers les plus verbeux

```logql
topk(3, sum by (container) (count_over_time({job="docker"}[1h])))
```

**Exercice 1.6** : Ratio d'erreurs

```logql
sum(rate({job="docker"} |~ "error|ERROR" [5m])) 
/ 
sum(rate({job="docker"}[5m]))
```

### âœ… CritÃ¨res de RÃ©ussite

- [ ] Vous pouvez afficher les logs de chaque container
- [ ] Vous savez filtrer par contenu (regex)
- [ ] Vous pouvez calculer des mÃ©triques depuis les logs
- [ ] Vous comprenez la diffÃ©rence entre log queries et metric queries

---

## ğŸ“Š Partie 4 : Dashboard de Logs (30 min)

### CrÃ©er un Dashboard de Monitoring de Logs

#### Panel 1 : Volume de Logs par Container

**Type** : Time series

**Query** :
```logql
sum by (container) (rate({job="docker"}[1m]))
```

**Configuration** :
- Legend : `{{container}}`
- Unit : logs/s
- Stacking : Normal

#### Panel 2 : Logs d'Erreur

**Type** : Stat

**Query** :
```logql
sum(count_over_time({job="docker"} |~ "error|ERROR|Error" [5m]))
```

**Configuration** :
- Color : Red si > 0
- Unit : short
- Thresholds : 0 (green), 1 (red)

#### Panel 3 : Logs en Temps RÃ©el

**Type** : Logs

**Query** :
```logql
{job="docker"}
```

**Configuration** :
- Show time : Yes
- Show labels : Yes
- Wrap lines : Yes
- Deduplication : None

#### Panel 4 : Top Containers par Volume

**Type** : Bar chart

**Query** :
```logql
topk(5, sum by (container) (count_over_time({job="docker"}[1h])))
```

**Configuration** :
- Orientation : Horizontal
- Legend : Hide
- Unit : short

#### Panel 5 : Timeline des Erreurs

**Type** : State timeline

**Query** :
```logql
sum by (container) (count_over_time({job="docker", level="ERROR"}[1m]))
```

**Configuration** :
- Show values : Never
- Color mode : Cell

### Variables de Dashboard

Ajoutez une variable pour sÃ©lectionner le container :

```yaml
Name: container
Type: Query
Datasource: Loki
Query: label_values(container)
Multi-value: Yes
Include All: Yes
```

Utilisez-la dans les requÃªtes :
```logql
{container=~"$container"}
```

---

## ğŸ”— Partie 5 : CorrÃ©lation Logs â†” MÃ©triques (30 min)

### Concept

Lier les logs aux mÃ©triques pour un troubleshooting efficace.

### MÃ©thode 1 : Data Links

Dans un panel de mÃ©triques Prometheus, ajoutez un lien vers les logs :

**Dashboard Settings â†’ Links â†’ Add link** :

```yaml
Title: View Logs
URL: /explore?orgId=1&left={"datasource":"Loki","queries":[{"expr":"{container=\"${__field.labels.instance}\"}"}]}
```

### MÃ©thode 2 : Annotations depuis Logs

CrÃ©ez une annotation pour afficher les erreurs sur les graphiques :

**Dashboard Settings â†’ Annotations â†’ Add annotation** :

```yaml
Name: Errors
Datasource: Loki
Query: {job="docker"} |~ "error|ERROR"
Text: {{container}}: {{__line}}
Tags: error
```

### MÃ©thode 3 : Panels CombinÃ©s

CrÃ©ez un dashboard avec mÃ©triques + logs cÃ´te Ã  cÃ´te :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CPU Usage (Prometheus)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Logs (Loki) - Filtered by time     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Utilisez le **time range sync** pour synchroniser les vues.

---

## ğŸ¯ Exercice Pratique 2 : Dashboard Complet (30 min)

### Objectif

CrÃ©er un dashboard "ObservabilitÃ© ComplÃ¨te" combinant mÃ©triques et logs.

### Structure

1. **Row 1 : MÃ©triques SystÃ¨me**
   - CPU Usage (Prometheus)
   - Memory Usage (Prometheus)
   - Disk I/O (Prometheus)

2. **Row 2 : Logs**
   - Volume de logs par service
   - Logs d'erreur (count)
   - Logs en temps rÃ©el (filtered)

3. **Row 3 : CorrÃ©lation**
   - Timeline des erreurs
   - Annotations sur les mÃ©triques

### RequÃªtes SuggÃ©rÃ©es

#### MÃ©triques (Prometheus)

```promql
# CPU
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Memory
(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100
```

#### Logs (Loki)

```logql
# Volume par service
sum by (container) (rate({job="docker"}[1m]))

# Erreurs
sum(count_over_time({job="docker"} |~ "error|ERROR" [5m]))

# Logs filtrÃ©s
{job="docker", container=~"$container"} |~ "$search"
```

### Variables

```yaml
# Container
Name: container
Query: label_values(container)

# Search
Name: search
Type: Text box
Default: ""
```

### âœ… CritÃ¨res de RÃ©ussite

- [ ] Dashboard avec 3 rows (MÃ©triques, Logs, CorrÃ©lation)
- [ ] Variables fonctionnelles (container, search)
- [ ] Data links entre mÃ©triques et logs
- [ ] Annotations d'erreurs visibles sur les graphiques

---

## ğŸš€ Partie 6 : Cas d'Usage AvancÃ©s (Bonus)

### 1. Alertes depuis Logs

CrÃ©ez une alerte sur le volume d'erreurs :

```logql
sum(rate({job="docker"} |~ "error|ERROR" [5m])) > 10
```

**Configuration** :
- Condition : `WHEN last() OF query(A) IS ABOVE 10`
- Evaluate every : 1m
- For : 5m

### 2. Logs StructurÃ©s (JSON)

Si vos applications loggent en JSON :

```json
{"level":"ERROR","message":"Database connection failed","user_id":123,"timestamp":"2024-10-27T00:00:00Z"}
```

RequÃªte :
```logql
{job="app"} 
| json 
| level="ERROR" 
| line_format "User {{.user_id}}: {{.message}}"
```

### 3. Logs Multi-Lignes

Pour les stack traces Java/Python :

```yaml
# promtail-config.yaml
pipeline_stages:
  - multiline:
      firstline: '^\d{4}-\d{2}-\d{2}'
      max_wait_time: 3s
```

### 4. Logs avec Contexte

Afficher les lignes avant/aprÃ¨s une erreur :

```logql
{job="app"} |~ "error" | context before=5 after=5
```

---

## ğŸ“– Ressources

### Documentation Officielle

- [Loki Documentation](https://grafana.com/docs/loki/latest/)
- [LogQL Guide](https://grafana.com/docs/loki/latest/logql/)
- [Promtail Configuration](https://grafana.com/docs/loki/latest/clients/promtail/configuration/)

### Cheat Sheets

- [LogQL Cheat Sheet](https://grafana.com/docs/loki/latest/logql/log_queries/)
- [Promtail Pipeline Stages](https://grafana.com/docs/loki/latest/clients/promtail/stages/)

### Exemples

- [LogQL Examples](https://grafana.com/docs/loki/latest/logql/query_examples/)
- [Grafana Loki Dashboards](https://grafana.com/grafana/dashboards/?search=loki)

---

## âœ… Checklist de Validation

Avant de passer au lab suivant, assurez-vous de :

- [ ] Loki et Promtail sont dÃ©marrÃ©s et fonctionnels
- [ ] Vous pouvez voir les logs dans Grafana Explore
- [ ] Vous maÃ®trisez les requÃªtes LogQL de base
- [ ] Vous savez filtrer et parser les logs
- [ ] Vous avez crÃ©Ã© un dashboard de logs
- [ ] Vous comprenez la corrÃ©lation logs â†” mÃ©triques
- [ ] Vous pouvez calculer des mÃ©triques depuis les logs

---

## ğŸ”™ Navigation

- [â¬…ï¸ Retour au Jour 2](../README.md)
- [â¡ï¸ Lab suivant : Lab 2.2 - Tempo (Tracing)](../Lab-2.2-Tempo/)
- [ğŸ  Accueil Formation](../../README-MAIN.md)

---

## ğŸ“ Points ClÃ©s Ã  Retenir

1. **Loki** = Prometheus pour les logs (indexation par labels uniquement)
2. **Promtail** = Agent de collecte configurable avec pipelines
3. **LogQL** = Langage similaire Ã  PromQL (log queries + metric queries)
4. **CorrÃ©lation** = Lier logs et mÃ©triques pour un troubleshooting efficace
5. **Ã‰conomique** = Stockage compressÃ©, pas d'indexation full-text

---

**ğŸ‰ FÃ©licitations !** Vous maÃ®trisez maintenant l'agrÃ©gation de logs avec Loki !

Passez au [Lab 2.2 - Tempo](../Lab-2.2-Tempo/) pour dÃ©couvrir le tracing distribuÃ©.
