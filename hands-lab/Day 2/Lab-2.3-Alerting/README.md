# üîî Lab 2.3 : Alerting Avanc√© avec Alertmanager

**Dur√©e estim√©e** : 2 heures  
**Niveau** : Interm√©diaire  
**Pr√©requis** : Lab 1.4 (Prometheus), Lab 2.1 (Loki)

---

## üéØ Objectifs d'Apprentissage

√Ä la fin de ce lab, vous serez capable de :

- ‚úÖ Configurer des r√®gles d'alerte dans Prometheus
- ‚úÖ Ma√Ætriser Alertmanager (routing, grouping, inhibition)
- ‚úÖ Cr√©er des alertes depuis Grafana (m√©triques et logs)
- ‚úÖ Configurer des notifications (email, Slack, webhook)
- ‚úÖ G√©rer le silence et l'inhibition des alertes
- ‚úÖ Cr√©er des templates d'alertes personnalis√©s

---

## üìã Pr√©requis

### Services Docker Requis

```bash
# V√©rifier que les services sont d√©marr√©s
docker ps | grep -E "prometheus|alertmanager|grafana"

# Devrait afficher :
# - prometheus (port 9090)
# - alertmanager (port 9093)
# - grafana (port 3000)
```

### Acc√®s aux Interfaces

| Service | URL | Credentials |
|---------|-----|-------------|
| **Grafana** | http://localhost:3000 | admin / GrafanaSecure123!Change@Me |
| **Prometheus** | http://localhost:9090 | - |
| **Alertmanager** | http://localhost:9093 | - |

---

## üìö Partie 1 : Architecture de l'Alerting (30 min)

### Composants

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Prometheus  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ Scrape ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Targets
‚îÇ             ‚îÇ
‚îÇ Alert Rules ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ Evaluate ‚îÄ‚îÄ‚ñ∂ Conditions
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ Fire Alert
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Alertmanager    ‚îÇ
‚îÇ                 ‚îÇ
‚îÇ - Grouping      ‚îÇ
‚îÇ - Routing       ‚îÇ
‚îÇ - Inhibition    ‚îÇ
‚îÇ - Silencing     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚ñ∂ Email
         ‚îú‚îÄ‚îÄ‚ñ∂ Slack
         ‚îú‚îÄ‚îÄ‚ñ∂ PagerDuty
         ‚îî‚îÄ‚îÄ‚ñ∂ Webhook
```

### Workflow

1. **Prometheus** : √âvalue les r√®gles d'alerte
2. **Alertmanager** : Re√ßoit les alertes et les traite
3. **Routing** : Dirige vers les bons receivers
4. **Grouping** : Regroupe les alertes similaires
5. **Notification** : Envoie aux canaux configur√©s

### √âtats d'une Alerte

| √âtat | Description | Dur√©e |
|------|-------------|-------|
| **Inactive** | Condition non remplie | - |
| **Pending** | Condition remplie, attente `for` | Configurable |
| **Firing** | Alerte active, envoy√©e √† Alertmanager | Jusqu'√† r√©solution |
| **Resolved** | Condition n'est plus remplie | - |

---

## üîß Partie 2 : R√®gles d'Alerte Prometheus (45 min)

### Structure d'une R√®gle

```yaml
groups:
  - name: example_alerts
    interval: 30s
    rules:
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          component: system
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value }}% on {{ $labels.instance }}"
```

### Composants d'une R√®gle

| Champ | Description | Exemple |
|-------|-------------|---------|
| **alert** | Nom de l'alerte | `HighCPUUsage` |
| **expr** | Requ√™te PromQL | `cpu_usage > 80` |
| **for** | Dur√©e avant firing | `5m` |
| **labels** | Labels ajout√©s | `severity: warning` |
| **annotations** | Informations descriptives | `summary`, `description` |

### Cr√©er des R√®gles d'Alerte

Cr√©ez le fichier `prometheus/rules/alerts.yml` :

```yaml
groups:
  # Alertes Syst√®me
  - name: system_alerts
    interval: 30s
    rules:
      # CPU
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          component: system
          team: infrastructure
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | humanize }}% (threshold: 80%)"
          runbook_url: "https://wiki.company.com/runbooks/high-cpu"

      - alert: CriticalCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
        for: 2m
        labels:
          severity: critical
          component: system
          team: infrastructure
        annotations:
          summary: "CRITICAL: CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | humanize }}% (threshold: 95%)"

      # Memory
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          component: system
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanize }}%"

      - alert: CriticalMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 95
        for: 2m
        labels:
          severity: critical
          component: system
        annotations:
          summary: "CRITICAL: Memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanize }}%"

      # Disk
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"} / node_filesystem_size_bytes) * 100 < 20
        for: 10m
        labels:
          severity: warning
          component: storage
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Only {{ $value | humanize }}% available on {{ $labels.mountpoint }}"

  # Alertes Services
  - name: service_alerts
    interval: 30s
    rules:
      # Service Down
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
          component: service
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute"

      # Prometheus
      - alert: PrometheusTargetMissing
        expr: up{job="prometheus"} == 0
        for: 1m
        labels:
          severity: critical
          component: monitoring
        annotations:
          summary: "Prometheus target missing"
          description: "Prometheus target {{ $labels.instance }} is down"

      # Grafana
      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 2m
        labels:
          severity: critical
          component: monitoring
        annotations:
          summary: "Grafana is down"
          description: "Grafana has been down for more than 2 minutes"

  # Alertes Application
  - name: application_alerts
    interval: 30s
    rules:
      # HTTP Errors
      - alert: HighHTTPErrorRate
        expr: (sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))) * 100 > 5
        for: 5m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "High HTTP error rate"
          description: "Error rate is {{ $value | humanize }}% (threshold: 5%)"

      # Response Time
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "High response time"
          description: "95th percentile response time is {{ $value | humanize }}s"
```

### Recharger les R√®gles

```bash
# Windows PowerShell
docker-compose restart prometheus

# V√©rifier les r√®gles dans Prometheus
# http://localhost:9090/rules
```

### Tester les R√®gles

```bash
# Acc√©der √† Prometheus
# http://localhost:9090/alerts

# V√©rifier l'√©tat des alertes :
# - Inactive (vert)
# - Pending (jaune)
# - Firing (rouge)
```

---

## üéØ Exercice Pratique 1 : Cr√©er des Alertes (30 min)

### Objectif

Cr√©er et tester des r√®gles d'alerte personnalis√©es.

### √âtapes

#### 1. Cr√©er une Alerte de Test

Ajoutez dans `prometheus/rules/alerts.yml` :

```yaml
- alert: AlwaysFiring
  expr: vector(1)
  for: 1m
  labels:
    severity: info
    component: test
  annotations:
    summary: "Test alert - Always firing"
    description: "This is a test alert that always fires"
```

#### 2. Recharger Prometheus

```bash
docker-compose restart prometheus
```

#### 3. V√©rifier dans Prometheus

```
http://localhost:9090/alerts
```

Vous devriez voir l'alerte passer de **Pending** √† **Firing** apr√®s 1 minute.

#### 4. V√©rifier dans Alertmanager

```
http://localhost:9093/#/alerts
```

L'alerte devrait appara√Ætre dans Alertmanager.

### ‚úÖ Crit√®res de R√©ussite

- [ ] L'alerte appara√Æt dans Prometheus
- [ ] L'alerte passe √† l'√©tat Firing apr√®s 1 minute
- [ ] L'alerte est visible dans Alertmanager

---

## üì® Partie 3 : Configuration Alertmanager (45 min)

### Architecture de Configuration

```yaml
route:           # Routing principal
  receiver:      # Receiver par d√©faut
  routes:        # Routes sp√©cifiques
    - match:     # Conditions de matching

receivers:       # D√©finition des receivers
  - name:        # Nom du receiver
    email_configs:
    slack_configs:
    webhook_configs:

inhibit_rules:   # R√®gles d'inhibition
templates:       # Templates personnalis√©s
```

### Configuration Compl√®te

√âditez `alertmanager/alertmanager.yml` :

```yaml
global:
  # Configuration SMTP (Email)
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alertmanager@company.com'
  smtp_auth_username: 'your-email@gmail.com'
  smtp_auth_password: 'your-app-password'
  smtp_require_tls: true

  # Configuration Slack
  slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

# Routing des alertes
route:
  # Grouping : regrouper les alertes similaires
  group_by: ['alertname', 'cluster', 'service']
  
  # Timing
  group_wait: 30s        # Attendre 30s avant d'envoyer un groupe
  group_interval: 5m     # Attendre 5m avant d'envoyer de nouvelles alertes du m√™me groupe
  repeat_interval: 4h    # R√©p√©ter toutes les 4h si non r√©solu

  # Receiver par d√©faut
  receiver: 'default'

  # Routes sp√©cifiques
  routes:
    # Alertes critiques ‚Üí Email + Slack
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 10s
      repeat_interval: 1h

    # Alertes warning ‚Üí Slack uniquement
    - match:
        severity: warning
      receiver: 'warning-alerts'
      repeat_interval: 12h

    # Alertes infrastructure ‚Üí √âquipe infra
    - match:
        team: infrastructure
      receiver: 'infra-team'

    # Alertes application ‚Üí √âquipe dev
    - match:
        component: application
      receiver: 'dev-team'

# D√©finition des receivers
receivers:
  # Default
  - name: 'default'
    webhook_configs:
      - url: 'http://localhost:5001/default'

  # Alertes critiques
  - name: 'critical-alerts'
    email_configs:
      - to: 'oncall@company.com'
        headers:
          Subject: 'üî¥ CRITICAL: {{ .GroupLabels.alertname }}'
        html: '{{ template "email.html" . }}'
    slack_configs:
      - channel: '#alerts-critical'
        title: 'üî¥ CRITICAL ALERT'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        send_resolved: true

  # Alertes warning
  - name: 'warning-alerts'
    slack_configs:
      - channel: '#alerts-warning'
        title: '‚ö†Ô∏è WARNING'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        send_resolved: true

  # √âquipe infrastructure
  - name: 'infra-team'
    email_configs:
      - to: 'infra-team@company.com'
    slack_configs:
      - channel: '#team-infra'

  # √âquipe d√©veloppement
  - name: 'dev-team'
    email_configs:
      - to: 'dev-team@company.com'
    slack_configs:
      - channel: '#team-dev'

# R√®gles d'inhibition
inhibit_rules:
  # Si une alerte critique est active, ne pas envoyer les warnings similaires
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance']

  # Si un service est down, ne pas alerter sur ses m√©triques
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '.*'
    equal: ['instance']

# Templates personnalis√©s
templates:
  - '/etc/alertmanager/templates/*.tmpl'
```

### Templates Personnalis√©s

Cr√©ez `alertmanager/templates/email.tmpl` :

```go
{{ define "email.html" }}
<!DOCTYPE html>
<html>
<head>
  <style>
    body { font-family: Arial, sans-serif; }
    .alert { padding: 15px; margin: 10px 0; border-radius: 5px; }
    .critical { background-color: #ffebee; border-left: 5px solid #f44336; }
    .warning { background-color: #fff3e0; border-left: 5px solid #ff9800; }
    .info { background-color: #e3f2fd; border-left: 5px solid #2196f3; }
  </style>
</head>
<body>
  <h2>üîî Alert Notification</h2>
  
  {{ range .Alerts }}
  <div class="alert {{ .Labels.severity }}">
    <h3>{{ .Labels.alertname }}</h3>
    <p><strong>Summary:</strong> {{ .Annotations.summary }}</p>
    <p><strong>Description:</strong> {{ .Annotations.description }}</p>
    <p><strong>Severity:</strong> {{ .Labels.severity }}</p>
    <p><strong>Instance:</strong> {{ .Labels.instance }}</p>
    <p><strong>Started:</strong> {{ .StartsAt }}</p>
    {{ if .EndsAt }}
    <p><strong>Ended:</strong> {{ .EndsAt }}</p>
    {{ end }}
  </div>
  {{ end }}
  
  <p><a href="http://localhost:9093">View in Alertmanager</a></p>
</body>
</html>
{{ end }}
```

### Recharger Alertmanager

```bash
# Windows PowerShell
docker-compose restart alertmanager

# V√©rifier les logs
docker logs alertmanager --tail 50
```

---

## üé® Partie 4 : Alertes Grafana (30 min)

### Types d'Alertes Grafana

1. **Alertes sur M√©triques** (Prometheus, InfluxDB, etc.)
2. **Alertes sur Logs** (Loki)
3. **Alertes Unifi√©es** (Grafana 8+)

### Cr√©er une Alerte depuis un Panel

#### 1. Cr√©er un Panel de M√©trique

```
Dashboard ‚Üí Add panel ‚Üí Add a new panel
```

**Query (Prometheus)** :
```promql
100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
```

#### 2. Configurer l'Alerte

```
Panel ‚Üí Alert tab ‚Üí Create alert rule from this panel
```

**Configuration** :
```yaml
Name: High CPU Usage (Grafana)
Evaluate every: 1m
For: 5m

Condition:
  WHEN last() OF query(A)
  IS ABOVE 80
```

#### 3. Ajouter des Notifications

```
Contact points ‚Üí Add contact point
```

**Exemple Slack** :
```yaml
Name: Slack Alerts
Type: Slack
Webhook URL: https://hooks.slack.com/services/YOUR/WEBHOOK/URL
```

**Exemple Email** :
```yaml
Name: Email Alerts
Type: Email
Addresses: oncall@company.com
```

#### 4. Cr√©er une Notification Policy

```
Notification policies ‚Üí Add policy
```

```yaml
Matching labels:
  severity = critical

Contact point: Slack Alerts
```

### Alertes sur Logs (Loki)

#### Cr√©er une Alerte sur Volume de Logs

**Query (Loki)** :
```logql
sum(rate({job="docker"} |~ "error|ERROR" [5m]))
```

**Alert Condition** :
```yaml
WHEN last() OF query(A)
IS ABOVE 10
```

**Annotations** :
```yaml
Summary: High error rate in logs
Description: {{ $value }} errors per second detected
```

---

## üéØ Exercice Pratique 2 : Alertes Compl√®tes (30 min)

### Objectif

Cr√©er un syst√®me d'alerting complet avec routing et notifications.

### √âtapes

#### 1. Cr√©er 3 R√®gles d'Alerte

Dans `prometheus/rules/alerts.yml` :

```yaml
groups:
  - name: lab_alerts
    interval: 30s
    rules:
      # Info
      - alert: TestInfo
        expr: vector(1)
        labels:
          severity: info
        annotations:
          summary: "Test info alert"

      # Warning
      - alert: TestWarning
        expr: vector(1)
        labels:
          severity: warning
        annotations:
          summary: "Test warning alert"

      # Critical
      - alert: TestCritical
        expr: vector(1)
        labels:
          severity: critical
        annotations:
          summary: "Test critical alert"
```

#### 2. Configurer le Routing

Dans `alertmanager/alertmanager.yml`, v√©rifiez les routes pour chaque severity.

#### 3. Tester le Routing

```bash
# Recharger
docker-compose restart prometheus alertmanager

# V√©rifier dans Alertmanager
# http://localhost:9093/#/alerts

# Les alertes doivent √™tre rout√©es selon leur severity
```

#### 4. Cr√©er une Alerte Grafana

- Panel : CPU Usage
- Condition : > 50%
- Contact point : Webhook (http://localhost:5001/grafana)

### ‚úÖ Crit√®res de R√©ussite

- [ ] 3 alertes cr√©√©es (info, warning, critical)
- [ ] Routing fonctionnel dans Alertmanager
- [ ] Alertes visibles dans Grafana
- [ ] Contact points configur√©s

---

## üîï Partie 5 : Silences et Inhibition (15 min)

### Cr√©er un Silence

#### Via Alertmanager UI

```
http://localhost:9093/#/silences ‚Üí New Silence
```

**Configuration** :
```yaml
Matchers:
  alertname = TestCritical

Duration: 2h
Creator: admin@company.com
Comment: Maintenance window
```

#### Via API

```bash
# Cr√©er un silence
curl -X POST http://localhost:9093/api/v2/silences \
  -H "Content-Type: application/json" \
  -d '{
    "matchers": [
      {
        "name": "alertname",
        "value": "TestCritical",
        "isRegex": false
      }
    ],
    "startsAt": "2024-10-27T00:00:00Z",
    "endsAt": "2024-10-27T02:00:00Z",
    "createdBy": "admin",
    "comment": "Maintenance window"
  }'
```

### R√®gles d'Inhibition

Les r√®gles d'inhibition emp√™chent certaines alertes d'√™tre envoy√©es si d'autres sont actives.

**Exemple** : Ne pas alerter sur CPU si le service est down

```yaml
inhibit_rules:
  - source_match:
      alertname: 'ServiceDown'
    target_match:
      alertname: 'HighCPUUsage'
    equal: ['instance']
```

---

## üìä Partie 6 : Dashboard d'Alerting (15 min)

### Cr√©er un Dashboard de Monitoring des Alertes

#### Panel 1 : Alertes Actives

**Type** : Stat

**Query (Prometheus)** :
```promql
ALERTS{alertstate="firing"}
```

**Configuration** :
- Color : Red
- Thresholds : 0 (green), 1 (red)

#### Panel 2 : Alertes par Severity

**Type** : Pie chart

**Queries** :
```promql
# Critical
count(ALERTS{alertstate="firing", severity="critical"})

# Warning
count(ALERTS{alertstate="firing", severity="warning"})

# Info
count(ALERTS{alertstate="firing", severity="info"})
```

#### Panel 3 : Timeline des Alertes

**Type** : State timeline

**Query** :
```promql
ALERTS{alertstate="firing"}
```

#### Panel 4 : Top Alertes

**Type** : Table

**Query** :
```promql
topk(10, count by (alertname) (ALERTS{alertstate="firing"}))
```

---

## üìñ Ressources

### Documentation Officielle

- [Prometheus Alerting](https://prometheus.io/docs/alerting/latest/overview/)
- [Alertmanager Configuration](https://prometheus.io/docs/alerting/latest/configuration/)
- [Grafana Alerting](https://grafana.com/docs/grafana/latest/alerting/)

### Exemples de R√®gles

- [Awesome Prometheus Alerts](https://awesome-prometheus-alerts.grep.to/)
- [Prometheus Alert Rules](https://github.com/samber/awesome-prometheus-alerts)

### Int√©grations

- [Slack Integration](https://prometheus.io/docs/alerting/latest/configuration/#slack_config)
- [Email Integration](https://prometheus.io/docs/alerting/latest/configuration/#email_config)
- [PagerDuty Integration](https://prometheus.io/docs/alerting/latest/configuration/#pagerduty_config)

---

## ‚úÖ Checklist de Validation

Avant de passer au lab suivant, assurez-vous de :

- [ ] Prometheus √©value correctement les r√®gles d'alerte
- [ ] Alertmanager re√ßoit et route les alertes
- [ ] Le routing fonctionne selon les labels
- [ ] Les silences fonctionnent
- [ ] Les r√®gles d'inhibition sont effectives
- [ ] Vous avez cr√©√© des alertes dans Grafana
- [ ] Les notifications sont configur√©es (m√™me en test)

---

## üîô Navigation

- [‚¨ÖÔ∏è Retour au Jour 2](../README.md)
- [‚û°Ô∏è Lab suivant : Lab 2.4 - Dashboards Avanc√©s](../Lab-2.4-Advanced-Dashboards/)
- [üè† Accueil Formation](../../README-MAIN.md)

---

## üéì Points Cl√©s √† Retenir

1. **Prometheus** √©value les r√®gles, **Alertmanager** g√®re le routing
2. **Grouping** r√©duit le bruit en regroupant les alertes similaires
3. **Inhibition** emp√™che les alertes redondantes
4. **Silences** permettent de d√©sactiver temporairement des alertes
5. **Labels** sont essentiels pour le routing et le grouping
6. **Grafana** offre une interface unifi√©e pour g√©rer toutes les alertes

---

**üéâ F√©licitations !** Vous ma√Ætrisez maintenant l'alerting avec Prometheus et Grafana !

Passez au [Lab 2.4 - Dashboards Avanc√©s](../Lab-2.4-Advanced-Dashboards/) pour approfondir vos comp√©tences.
